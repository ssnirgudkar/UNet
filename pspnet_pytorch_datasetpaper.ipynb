{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pspnet-pytorch-datasetpaper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtzg9v9tnbNSXIp+yfnvOX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/UNet/blob/main/pspnet_pytorch_datasetpaper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYlYx-tlnrte"
      },
      "source": [
        "#Encoder - se_resnext50_32x4d, Decoder - FPN, 100 epochs. FPN = next generation pspnet "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-T-DGiWU3kt"
      },
      "source": [
        "https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb\n",
        "\n",
        "** our masked image is gray scale with 1 channel (height * width*channel=1). and the pixel values are equal to the class ids. Each pixle value is only 1 value and not (a*b*c). The CV2.imread, with a flag od 0 that reads it as a gray scalae does not work for our images although it works for the other dataset\n",
        "\n",
        "## image size: (256, 320) #it's height and width\n",
        "\n",
        "## Use GPU so that CUDA is available\n",
        "\n",
        "1. train - 2570, val - 740, test - 370 , Epochs - 100 \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time -   hrs. Testing time -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11nEKAj4mXcL"
      },
      "source": [
        "#To Do - \n",
        "1. Right now dice loss and iau for accuracy. check if we need to select any other metrics \n",
        "2. We are not plotting the losses / accuracy. need to check what to use to get the plots. should that be consistent across all the models? in that case should we use tensorboard?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWlqTikUn49F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe27016-e82d-464e-9c4f-9102ad0c8fb4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 29 09:50:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBdE7JFgimEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89f8faa-1d3f-47bb-c0f5-e1f14c74a8c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWmjSDMi34CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ec04a9-de1e-47fe-89a4-52e2c3b19a42"
      },
      "source": [
        "# Install required libs\n",
        "!pip install -U segmentation-models-pytorch albumentations --user"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.10.0+cu111)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.9.0+cu111)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (7.1.2)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.4.58-cp37-cp37m-manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 67 kB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (0.22.2.post1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.0.1)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=7928d40e787abb454386c09ff47d44f17e03b8fa2953c0fe5eb99b179dd0de9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=fd384a5f8191b066c016a19a7371fbaebfe2eb266856652604f3ec65de4e3fc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: opencv-python-headless, munch, timm, qudida, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch, albumentations\n",
            "Successfully installed albumentations-1.1.0 efficientnet-pytorch-0.6.3 munch-2.5.0 opencv-python-headless-4.5.4.58 pretrainedmodels-0.7.4 qudida-0.0.4 segmentation-models-pytorch-0.2.0 timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bshuhFbf4HRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba65b8c-b75b-4bc7-c7a1-02e835100815"
      },
      "source": [
        "!pip uninstall -y segmentation-models-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: segmentation-models-pytorch 0.2.0\n",
            "Uninstalling segmentation-models-pytorch-0.2.0:\n",
            "  Successfully uninstalled segmentation-models-pytorch-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz0adR3tUZVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a497315-c360-408b-d1f0-dd78447a3ac5"
      },
      "source": [
        "!git clone https://github.com/Cadene/pretrained-models.pytorch.git\n",
        "!git clone https://github.com/qubvel/segmentation_models.pytorch\n",
        "!git clone https://github.com/alexgkendall/SegNet-Tutorial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pretrained-models.pytorch'...\n",
            "remote: Enumerating objects: 803, done.\u001b[K\n",
            "remote: Total 803 (delta 0), reused 0 (delta 0), pack-reused 803\u001b[K\n",
            "Receiving objects: 100% (803/803), 522.66 KiB | 11.88 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n",
            "Cloning into 'segmentation_models.pytorch'...\n",
            "remote: Enumerating objects: 1517, done.\u001b[K\n",
            "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
            "remote: Total 1517 (delta 142), reused 137 (delta 87), pack-reused 1277\u001b[K\n",
            "Receiving objects: 100% (1517/1517), 4.90 MiB | 18.17 MiB/s, done.\n",
            "Resolving deltas: 100% (825/825), done.\n",
            "Cloning into 'SegNet-Tutorial'...\n",
            "remote: Enumerating objects: 2785, done.\u001b[K\n",
            "remote: Total 2785 (delta 0), reused 0 (delta 0), pack-reused 2785\u001b[K\n",
            "Receiving objects: 100% (2785/2785), 340.84 MiB | 28.06 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7thoJfVhVlga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8665305b-e01f-4f25-c89f-357c9f58f632"
      },
      "source": [
        "#!pip install -U segmentation-models-pytorch albumentations --user\n",
        "#!pip uninstall -y segmentation-models-pytorch\n",
        "#!pip install segmentation_models_pytorch\n",
        "\n",
        "!pip install git+https://github.com/IvyGongoogle/pretrained-models.pytorch\n",
        "!pip install git+https://github.com/lukemelas/EfficientNet-PyTorch\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/IvyGongoogle/pretrained-models.pytorch\n",
            "  Cloning https://github.com/IvyGongoogle/pretrained-models.pytorch to /tmp/pip-req-build-r3eq9wxp\n",
            "  Running command git clone -q https://github.com/IvyGongoogle/pretrained-models.pytorch /tmp/pip-req-build-r3eq9wxp\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.0) (0.10.0+cu111)\n",
            "Requirement already satisfied: munch in /root/.local/lib/python3.7/site-packages (from pretrainedmodels==0.7.0) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.0) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels==0.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels==0.7.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels==0.7.0) (1.19.5)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.0-py3-none-any.whl size=60553 sha256=4e8075b8509859672137b5fa959b7d73f20f8b0bc7c8e72e6af94e2ee1a4a04c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c81ax9j9/wheels/a8/51/08/bb41925a23e2225b4ca505b4e1709ce700613282945986a4b0\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: pretrainedmodels\n",
            "  Attempting uninstall: pretrainedmodels\n",
            "    Found existing installation: pretrainedmodels 0.7.4\n",
            "    Uninstalling pretrainedmodels-0.7.4:\n",
            "      Successfully uninstalled pretrainedmodels-0.7.4\n",
            "Successfully installed pretrainedmodels-0.7.0\n",
            "Collecting git+https://github.com/lukemelas/EfficientNet-PyTorch\n",
            "  Cloning https://github.com/lukemelas/EfficientNet-PyTorch to /tmp/pip-req-build-ivuw6xi5\n",
            "  Running command git clone -q https://github.com/lukemelas/EfficientNet-PyTorch /tmp/pip-req-build-ivuw6xi5\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.7.1) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.7.1) (3.7.4.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=20622 sha256=6008d91a972371b981170cbd127d53563312be5191e5fbf5e37c067ab290c1fc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-58lnupb5/wheels/f2/33/a5/85d7a0f3b00e37fc1cbe80d6edb12baeee3f7eddc9c881938c\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "  Attempting uninstall: efficientnet-pytorch\n",
            "    Found existing installation: efficientnet-pytorch 0.6.3\n",
            "    Uninstalling efficientnet-pytorch-0.6.3:\n",
            "      Successfully uninstalled efficientnet-pytorch-0.6.3\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u3NegrvTlOl"
      },
      "source": [
        "#!pip install efficientnet-pytorch\n",
        "#!pip install pretrainedmodels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3_Ebd77C2Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a05e2bf-3b3c-4070-b44a-5c1d5943e6b5"
      },
      "source": [
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
            "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-621hwehe\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-621hwehe\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.0) (0.10.0+cu111)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Using cached pretrainedmodels-0.7.4-py3-none-any.whl\n",
            "Collecting efficientnet-pytorch==0.6.3\n",
            "  Using cached efficientnet_pytorch-0.6.3-py3-none-any.whl\n",
            "Requirement already satisfied: timm==0.4.12 in /root/.local/lib/python3.7/site-packages (from segmentation-models-pytorch==0.2.0) (0.4.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (4.62.3)\n",
            "Requirement already satisfied: munch in /root/.local/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (1.15.0)\n",
            "Building wheels for collected packages: segmentation-models-pytorch\n",
            "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.2.0-py3-none-any.whl size=88601 sha256=4d5364bbb208898c0730d0d5ad052fac50d7a2eb1a6573e15da368fc592f94c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_fi2sojx/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n",
            "Successfully built segmentation-models-pytorch\n",
            "Installing collected packages: pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "  Attempting uninstall: pretrainedmodels\n",
            "    Found existing installation: pretrainedmodels 0.7.0\n",
            "    Uninstalling pretrainedmodels-0.7.0:\n",
            "      Successfully uninstalled pretrainedmodels-0.7.0\n",
            "  Attempting uninstall: efficientnet-pytorch\n",
            "    Found existing installation: efficientnet-pytorch 0.7.1\n",
            "    Uninstalling efficientnet-pytorch-0.7.1:\n",
            "      Successfully uninstalled efficientnet-pytorch-0.7.1\n",
            "Successfully installed efficientnet-pytorch-0.6.3 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9xamsKiE9c6"
      },
      "source": [
        "# Restart Runtime now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIUWW0c6IJN"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCpIpta03RyH"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIQFK_CNoJa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88679ef-4ba0-4644-91b0-8dd438f2bf06"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8jJFC435wa"
      },
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/TheIRDataset'\n",
        "#DATA_DIR = '/content/drive/MyDrive/psp-1.0'\n",
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print('Loading data...')\n",
        "    os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')\n",
        "    print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyxerGLW4FLz"
      },
      "source": [
        "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'testannot')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvu2KtF4gzr"
      },
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex_P5Fvi4mF4"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLwpyW_P4vtk"
      },
      "source": [
        "class Dataset(BaseDataset):\n",
        "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    #0 - sky(d.blue). 1 - water(l.blue).   2 - bridge(yellow).   3 - obstacle(purple).  4- living ob(green).  \n",
        "    #                5- backgnd (orange). 6 - self(pink)\n",
        "    \n",
        "    CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "                  \n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            classes=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "        \n",
        "        #mask1 = cv2.imread(self.masks_fps[1], cv2.IMREAD_UNCHANGED)\n",
        "        #print(\"true value of image\", np.unique(mask1))\n",
        "        # convert str names to class values on masks\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "        \n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        #HAD TO CHANGE FLAG 0 TO IM_READ_UNCHANGED BELOW as it was not reading the pixel values otherwise \n",
        "        # mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_UNCHANGED)\n",
        "        #print (\"masked image values\", mask)\n",
        "        #print (\"masked shape values\", mask.shape)\n",
        "        #print(\"masked unique values\", np.unique(mask))\n",
        "        #print(\"pixel values of mask\", mask)\n",
        "        \n",
        "        # extract certain classes from mask (e.g. cars)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        #print (\"masks extraction\", masks)\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "        #print(\"stacks of classes for mask\", masks)\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            #print('masked array first', np.unique(mask))\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7UyZe2T5FWv"
      },
      "source": [
        "# Lets look at data we have. it can only put 1 class at a time \n",
        "\n",
        "dataset = Dataset(x_train_dir, y_train_dir, classes=['background'])\n",
        "\n",
        "image, mask = dataset[2569] # get some sample. \n",
        "\n",
        "\n",
        "'''\n",
        "#just picking 1 image from the train dataset. Also selcting only class 'obstacle'. \n",
        "#the stacks of classes change when i change the class to sky. so the visualize is only showing the array for that specific class. \n",
        "# it's bollean - yes or No. Sp it will say if every pixel belongs to that class or not. it's able to print the ground truth properly\n",
        "which means dataset is created properly. \n",
        "'''\n",
        "#print ('masked array' , np.unique(mask))\n",
        "visualize(\n",
        "    image=image, \n",
        "    cars_mask=mask.squeeze(),\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHskJX8u6foc"
      },
      "source": [
        "import albumentations as albu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OSF_x2YHu9p"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG-uG-R06kNm"
      },
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
        "\n",
        "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
        "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
        "\n",
        "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
        "        albu.IAAPerspective(p=0.5),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.CLAHE(p=1),\n",
        "                albu.RandomBrightness(p=1),\n",
        "                albu.RandomGamma(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.IAASharpen(p=1),\n",
        "                albu.Blur(blur_limit=3, p=1),\n",
        "                albu.MotionBlur(blur_limit=3, p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.RandomContrast(p=1),\n",
        "                albu.HueSaturationValue(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.PadIfNeeded(384, 480)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "\n",
        "'''\n",
        "OpenCV img = cv2.imread(path) loads an image with HWC-layout (height, width, channels), \n",
        "while Pytorch requires CHW-layout. So we have to do np.transpose(image,(2,0,1))\n",
        "for HWC->CHW transformation.\n",
        "'''\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUqMy0pc6qAj"
      },
      "source": [
        "#### Visualize resulted augmented images and masks - thows error after 1st picture. \n",
        "#THIS IS NOT NECESSARY. plus our images have some augmentation\n",
        "'''\n",
        "augmented_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    augmentation=get_training_augmentation(), \n",
        "    classes=['obstacle'],\n",
        ")\n",
        "\n",
        "# same image with different random transforms\n",
        "#for i in range(3):  (Had to comment this out as i only have 1 image. will remove during actual training)\n",
        "for i in range(3):\n",
        "   image, mask = augmented_dataset[1]\n",
        "   visualize(image=image, mask=mask.squeeze(-1))\n",
        "   '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2rVoCdu_u5r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "42a4078f-99b4-407b-bc04-7a777be318f0"
      },
      "source": [
        "ENCODER = 'se_resnext50_32x4d'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "ACTIVATION = 'softmax2d' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.FPN(\n",
        "    encoder_name=ENCODER, \n",
        "    #encoder_weights=ENCODER_WEIGHTS, Do not want to use existing encoder weights. \n",
        "    encoder_weights=None, \n",
        "    classes=len(CLASSES), \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
        "'''if i use existing encoder weights, then it doesn't display all classes in the prediction for some reason. \n",
        "    have not found out exactly why. but perhaps std classes as sky / water are being taken up from the weights. \n",
        "    obstacle , backngd are not standard and therefore may not be plotted. So i will train the model from scratch\n",
        "''' \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"if i use existing encoder weights, then it doesn't display all classes in the prediction for some reason. \\n    have not found out exactly why. but perhaps std classes as sky / water are being taken up from the weights. \\n    obstacle , backngd are not standard and therefore may not be plotted. So i will train the model from scratch\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwDZ6rvsyowG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "1607bdae-8209-4d3c-d5f8-64ce76915696"
      },
      "source": [
        "train_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    #augmentation=get_training_augmentation(), try w/o augmentation \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir, \n",
        "    y_valid_dir, \n",
        "    #augmentation=get_validation_augmentation(), try w/o augmentation\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=4)\n",
        "#train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=12) - used for running iwth 1 image\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=5, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-eb4747d54c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#augmentation=get_training_augmentation(), try w/o augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0e2bbdd4c35e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, images_dir, masks_dir, classes, augmentation, preprocessing)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     ):\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzS1HykysUz"
      },
      "source": [
        "\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.0001),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPhMq-P4yvZD"
      },
      "source": [
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f50NIzkvy0Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ff516a-e7ad-4933-b232-e4623083febd"
      },
      "source": [
        "###### THIS IS WHERE YOU ARE TRAINING THE MODEL. ONLY RUN IF YOU WANT TO TRAIN\n",
        "max_score = 0\n",
        "\n",
        "for i in range(0, 100):\n",
        "    \n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "    \n",
        "    # do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, '/content/drive/MyDrive/Models/pspnet/pspnet-pytorch/best_model.pth')\n",
        "        print('Model saved!')\n",
        "        \n",
        "    if i == 25:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "        print('Decrease decoder learning rate to 1e-5!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train:   0%|          | 0/257 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 257/257 [03:40<00:00,  1.17it/s, dice_loss - 0.1198, iou_score - 0.806]\n",
            "valid: 100%|██████████| 148/148 [04:25<00:00,  1.79s/it, dice_loss - 0.0755, iou_score - 0.8636]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 1\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.06675, iou_score - 0.8775]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.72it/s, dice_loss - 0.07363, iou_score - 0.8664]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.88it/s, dice_loss - 0.06264, iou_score - 0.8844]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.75it/s, dice_loss - 0.06658, iou_score - 0.8783]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 3\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.05642, iou_score - 0.8952]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.54it/s, dice_loss - 0.06184, iou_score - 0.8867]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 4\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.87it/s, dice_loss - 0.05305, iou_score - 0.9011]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.79it/s, dice_loss - 0.05906, iou_score - 0.8914]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 5\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.88it/s, dice_loss - 0.05094, iou_score - 0.9047]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.73it/s, dice_loss - 0.0589, iou_score - 0.8916]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 6\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.87it/s, dice_loss - 0.0476, iou_score - 0.911]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.49it/s, dice_loss - 0.04939, iou_score - 0.9086]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 7\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.04326, iou_score - 0.9189]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.68it/s, dice_loss - 0.049, iou_score - 0.9094]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 8\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.04396, iou_score - 0.9176]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.66it/s, dice_loss - 0.04726, iou_score - 0.9124]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 9\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.04097, iou_score - 0.923]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.68it/s, dice_loss - 0.04708, iou_score - 0.9126]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 10\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.0396, iou_score - 0.9256]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.77it/s, dice_loss - 0.04558, iou_score - 0.9156]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 11\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.03623, iou_score - 0.9319]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.05it/s, dice_loss - 0.04111, iou_score - 0.9233]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 12\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.03278, iou_score - 0.938]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.15it/s, dice_loss - 0.04297, iou_score - 0.9197]\n",
            "\n",
            "Epoch: 13\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02966, iou_score - 0.9437]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.83it/s, dice_loss - 0.0353, iou_score - 0.9336]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 14\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02675, iou_score - 0.9491]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.08it/s, dice_loss - 0.03375, iou_score - 0.9365]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 15\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02525, iou_score - 0.9519]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.03it/s, dice_loss - 0.03291, iou_score - 0.9381]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 16\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.02408, iou_score - 0.9541]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.93it/s, dice_loss - 0.03103, iou_score - 0.9416]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 17\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02353, iou_score - 0.9552]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.05it/s, dice_loss - 0.03477, iou_score - 0.9352]\n",
            "\n",
            "Epoch: 18\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.93it/s, dice_loss - 0.0244, iou_score - 0.9535]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.77it/s, dice_loss - 0.03322, iou_score - 0.9378]\n",
            "\n",
            "Epoch: 19\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.0223, iou_score - 0.9575]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.83it/s, dice_loss - 0.03044, iou_score - 0.9426]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 20\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.93it/s, dice_loss - 0.02155, iou_score - 0.9589]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.97it/s, dice_loss - 0.03116, iou_score - 0.9414]\n",
            "\n",
            "Epoch: 21\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02079, iou_score - 0.9603]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.96it/s, dice_loss - 0.02958, iou_score - 0.9442]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 22\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02006, iou_score - 0.9616]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.96it/s, dice_loss - 0.02884, iou_score - 0.9455]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 23\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02086, iou_score - 0.9601]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.77it/s, dice_loss - 0.03781, iou_score - 0.9289]\n",
            "\n",
            "Epoch: 24\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02738, iou_score - 0.9482]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.07it/s, dice_loss - 0.03136, iou_score - 0.941]\n",
            "\n",
            "Epoch: 25\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.02039, iou_score - 0.961]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.88it/s, dice_loss - 0.02846, iou_score - 0.9464]\n",
            "Model saved!\n",
            "Decrease decoder learning rate to 1e-5!\n",
            "\n",
            "Epoch: 26\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01864, iou_score - 0.9643]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.90it/s, dice_loss - 0.02759, iou_score - 0.9479]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 27\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01827, iou_score - 0.965]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.90it/s, dice_loss - 0.02738, iou_score - 0.9482]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 28\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.93it/s, dice_loss - 0.01802, iou_score - 0.9655]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.99it/s, dice_loss - 0.02725, iou_score - 0.9485]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 29\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01781, iou_score - 0.9659]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.99it/s, dice_loss - 0.02715, iou_score - 0.9487]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 30\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01761, iou_score - 0.9663]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.74it/s, dice_loss - 0.02721, iou_score - 0.9485]\n",
            "\n",
            "Epoch: 31\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.93it/s, dice_loss - 0.01747, iou_score - 0.9666]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.89it/s, dice_loss - 0.02697, iou_score - 0.949]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 32\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01734, iou_score - 0.9668]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.14it/s, dice_loss - 0.02696, iou_score - 0.949]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 33\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01714, iou_score - 0.9672]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.06it/s, dice_loss - 0.02706, iou_score - 0.9488]\n",
            "\n",
            "Epoch: 34\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.93it/s, dice_loss - 0.01698, iou_score - 0.9675]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.15it/s, dice_loss - 0.02683, iou_score - 0.9493]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 35\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01681, iou_score - 0.9678]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.17it/s, dice_loss - 0.02693, iou_score - 0.949]\n",
            "\n",
            "Epoch: 36\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01671, iou_score - 0.968]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.88it/s, dice_loss - 0.02686, iou_score - 0.9492]\n",
            "\n",
            "Epoch: 37\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01648, iou_score - 0.9685]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.08it/s, dice_loss - 0.02688, iou_score - 0.9491]\n",
            "\n",
            "Epoch: 38\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01626, iou_score - 0.9689]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.94it/s, dice_loss - 0.02676, iou_score - 0.9494]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 39\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01615, iou_score - 0.9691]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.95it/s, dice_loss - 0.02679, iou_score - 0.9493]\n",
            "\n",
            "Epoch: 40\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01596, iou_score - 0.9695]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.91it/s, dice_loss - 0.02666, iou_score - 0.9495]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 41\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.93it/s, dice_loss - 0.01585, iou_score - 0.9697]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.99it/s, dice_loss - 0.02671, iou_score - 0.9494]\n",
            "\n",
            "Epoch: 42\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01574, iou_score - 0.9699]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.94it/s, dice_loss - 0.02666, iou_score - 0.9495]\n",
            "\n",
            "Epoch: 43\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01562, iou_score - 0.9701]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.71it/s, dice_loss - 0.02674, iou_score - 0.9494]\n",
            "\n",
            "Epoch: 44\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01553, iou_score - 0.9703]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.89it/s, dice_loss - 0.02663, iou_score - 0.9495]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 45\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01539, iou_score - 0.9705]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.88it/s, dice_loss - 0.02662, iou_score - 0.9496]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 46\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01526, iou_score - 0.9708]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.93it/s, dice_loss - 0.02646, iou_score - 0.9498]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 47\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01519, iou_score - 0.9709]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.96it/s, dice_loss - 0.02655, iou_score - 0.9497]\n",
            "\n",
            "Epoch: 48\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.93it/s, dice_loss - 0.01512, iou_score - 0.9711]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.94it/s, dice_loss - 0.02678, iou_score - 0.9492]\n",
            "\n",
            "Epoch: 49\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.015, iou_score - 0.9713]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.92it/s, dice_loss - 0.02645, iou_score - 0.9499]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 50\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.0149, iou_score - 0.9715]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.99it/s, dice_loss - 0.02649, iou_score - 0.9498]\n",
            "\n",
            "Epoch: 51\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01481, iou_score - 0.9716]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 12.05it/s, dice_loss - 0.02644, iou_score - 0.9499]\n",
            "\n",
            "Epoch: 52\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01474, iou_score - 0.9718]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.65it/s, dice_loss - 0.02635, iou_score - 0.95]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 53\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01467, iou_score - 0.9719]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.61it/s, dice_loss - 0.02632, iou_score - 0.9501]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 54\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01459, iou_score - 0.9721]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.99it/s, dice_loss - 0.0264, iou_score - 0.95]\n",
            "\n",
            "Epoch: 55\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.0145, iou_score - 0.9722]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.86it/s, dice_loss - 0.02641, iou_score - 0.9499]\n",
            "\n",
            "Epoch: 56\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01444, iou_score - 0.9723]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.73it/s, dice_loss - 0.02645, iou_score - 0.9498]\n",
            "\n",
            "Epoch: 57\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01431, iou_score - 0.9726]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.55it/s, dice_loss - 0.02629, iou_score - 0.9501]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 58\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01421, iou_score - 0.9728]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.80it/s, dice_loss - 0.02629, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 59\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01416, iou_score - 0.9729]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.68it/s, dice_loss - 0.02632, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 60\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01396, iou_score - 0.9733]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.71it/s, dice_loss - 0.02629, iou_score - 0.9501]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 61\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01391, iou_score - 0.9734]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.75it/s, dice_loss - 0.02612, iou_score - 0.9505]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 62\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01385, iou_score - 0.9735]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.72it/s, dice_loss - 0.02632, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 63\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01377, iou_score - 0.9737]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.80it/s, dice_loss - 0.02644, iou_score - 0.9499]\n",
            "\n",
            "Epoch: 64\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01376, iou_score - 0.9737]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.64it/s, dice_loss - 0.02631, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 65\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01364, iou_score - 0.9739]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.54it/s, dice_loss - 0.02622, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 66\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01356, iou_score - 0.974]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.82it/s, dice_loss - 0.02622, iou_score - 0.9503]\n",
            "\n",
            "Epoch: 67\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01345, iou_score - 0.9742]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.75it/s, dice_loss - 0.02613, iou_score - 0.9504]\n",
            "\n",
            "Epoch: 68\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01344, iou_score - 0.9743]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.74it/s, dice_loss - 0.02619, iou_score - 0.9503]\n",
            "\n",
            "Epoch: 69\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01343, iou_score - 0.9743]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.52it/s, dice_loss - 0.02628, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 70\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01342, iou_score - 0.9743]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.67it/s, dice_loss - 0.02628, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 71\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01362, iou_score - 0.9739]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.78it/s, dice_loss - 0.02629, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 72\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01341, iou_score - 0.9743]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.62it/s, dice_loss - 0.02639, iou_score - 0.9499]\n",
            "\n",
            "Epoch: 73\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.01331, iou_score - 0.9745]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.56it/s, dice_loss - 0.02636, iou_score - 0.95]\n",
            "\n",
            "Epoch: 74\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.0133, iou_score - 0.9745]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.51it/s, dice_loss - 0.02625, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 75\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01322, iou_score - 0.9747]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.74it/s, dice_loss - 0.02628, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 76\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01314, iou_score - 0.9748]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.59it/s, dice_loss - 0.02631, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 77\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01309, iou_score - 0.9749]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.83it/s, dice_loss - 0.02626, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 78\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01302, iou_score - 0.9751]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.74it/s, dice_loss - 0.02624, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 79\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01302, iou_score - 0.9751]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.82it/s, dice_loss - 0.02633, iou_score - 0.95]\n",
            "\n",
            "Epoch: 80\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.01296, iou_score - 0.9752]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.58it/s, dice_loss - 0.02635, iou_score - 0.95]\n",
            "\n",
            "Epoch: 81\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.0129, iou_score - 0.9753]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.81it/s, dice_loss - 0.02607, iou_score - 0.9505]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 82\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.0129, iou_score - 0.9753]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.83it/s, dice_loss - 0.02618, iou_score - 0.9503]\n",
            "\n",
            "Epoch: 83\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01277, iou_score - 0.9755]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.78it/s, dice_loss - 0.02577, iou_score - 0.9511]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 84\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.01267, iou_score - 0.9757]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.94it/s, dice_loss - 0.02609, iou_score - 0.9504]\n",
            "\n",
            "Epoch: 85\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.91it/s, dice_loss - 0.01264, iou_score - 0.9758]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.76it/s, dice_loss - 0.02619, iou_score - 0.9503]\n",
            "\n",
            "Epoch: 86\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.92it/s, dice_loss - 0.01265, iou_score - 0.9758]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.57it/s, dice_loss - 0.02617, iou_score - 0.9503]\n",
            "\n",
            "Epoch: 87\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01255, iou_score - 0.9759]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.54it/s, dice_loss - 0.02607, iou_score - 0.9505]\n",
            "\n",
            "Epoch: 88\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01251, iou_score - 0.976]\n",
            "valid: 100%|██████████| 148/148 [00:13<00:00, 11.32it/s, dice_loss - 0.02614, iou_score - 0.9503]\n",
            "\n",
            "Epoch: 89\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.01241, iou_score - 0.9762]\n",
            "valid: 100%|██████████| 148/148 [00:13<00:00, 11.31it/s, dice_loss - 0.02623, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 90\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.01244, iou_score - 0.9762]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.47it/s, dice_loss - 0.02621, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 91\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01235, iou_score - 0.9763]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.39it/s, dice_loss - 0.02623, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 92\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.90it/s, dice_loss - 0.01233, iou_score - 0.9764]\n",
            "valid: 100%|██████████| 148/148 [00:13<00:00, 11.38it/s, dice_loss - 0.02607, iou_score - 0.9505]\n",
            "\n",
            "Epoch: 93\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.88it/s, dice_loss - 0.01226, iou_score - 0.9765]\n",
            "valid: 100%|██████████| 148/148 [00:13<00:00, 11.22it/s, dice_loss - 0.02628, iou_score - 0.9501]\n",
            "\n",
            "Epoch: 94\n",
            "train: 100%|██████████| 257/257 [01:05<00:00,  3.89it/s, dice_loss - 0.01236, iou_score - 0.9763]\n",
            "valid: 100%|██████████| 148/148 [00:13<00:00, 11.21it/s, dice_loss - 0.02622, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 95\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.01235, iou_score - 0.9763]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.40it/s, dice_loss - 0.02621, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 96\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.87it/s, dice_loss - 0.01216, iou_score - 0.9767]\n",
            "valid: 100%|██████████| 148/148 [00:13<00:00, 11.21it/s, dice_loss - 0.02621, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 97\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.88it/s, dice_loss - 0.01203, iou_score - 0.9769]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.41it/s, dice_loss - 0.02614, iou_score - 0.9504]\n",
            "\n",
            "Epoch: 98\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.88it/s, dice_loss - 0.01194, iou_score - 0.9771]\n",
            "valid: 100%|██████████| 148/148 [00:12<00:00, 11.39it/s, dice_loss - 0.0262, iou_score - 0.9502]\n",
            "\n",
            "Epoch: 99\n",
            "train: 100%|██████████| 257/257 [01:06<00:00,  3.89it/s, dice_loss - 0.01209, iou_score - 0.9768]\n",
            "valid: 100%|██████████| 148/148 [00:13<00:00, 11.28it/s, dice_loss - 0.02646, iou_score - 0.9498]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvUHOusgy3-2"
      },
      "source": [
        "### THIS IS WHERE YOU LOAD THE MODEL FOR YOUR USE. FOR INFERENCE YOU CAN JUST START WITH THIS W/O TRAINING\n",
        "best_model = torch.load('/content/drive/MyDrive/Models/pspnet/pspnet-pytorch/best_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXdduyOcy7pI"
      },
      "source": [
        "# create test dataset\n",
        "test_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    #x_train_dir, - check with trained images to see if that gives any prediction\n",
        "    #y_train_dir,\n",
        "    #augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        "    \n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0sxWoPLy-VN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33817444-eff7-4914-ff92-75f5bce55026"
      },
      "source": [
        "## evaluate model on test set. THIS IS WHERE YOU ARE TESTING THE TEST DATA SET ON THE MODEL\n",
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model=best_model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "logs = test_epoch.run(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid:   0%|          | 1/370 [00:03<19:47,  3.22s/it, dice_loss - 0.01356, iou_score - 0.9734]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid: 100%|██████████| 370/370 [03:58<00:00,  1.55it/s, dice_loss - 0.02212, iou_score - 0.9586]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfeP4lx8zA88"
      },
      "source": [
        "# test dataset without transformations for image visualization\n",
        "test_dataset_vis = Dataset(\n",
        "    x_test_dir, y_test_dir, \n",
        "    #x_train_dir, y_train_dir, - checking with train dataset \n",
        "    classes=CLASSES,\n",
        "    \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgQgkpxiptDY"
      },
      "source": [
        "from torchvision.utils import save_image\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJBflBNqzDRo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "b80298f4-da0b-4457-d801-0d67f6f6bfde"
      },
      "source": [
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "import numpy as np \n",
        "DEVICE = 'cuda'\n",
        "for i in range(1):\n",
        "    n = np.random.choice(len(test_dataset))\n",
        "    print(\"Name of the image being predicted\",os.listdir(x_test_dir)[n])\n",
        "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[n]\n",
        "        \n",
        "    #image_vis = test_dataset_vis[10][0].astype('uint8')\n",
        "    #image, gt_mask = test_dataset[10]\n",
        "    \n",
        "    #print('gt mask before squeeze', np.unique(gt_mask))\n",
        "    \n",
        "    gt_mask = gt_mask.squeeze()\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    #x_tensor = torch.from_numpy(image).unsqueeze(0)\n",
        "    pr_mask = best_model.predict(x_tensor)\n",
        "    pr_mask1 = (pr_mask.squeeze().cpu().numpy().round())\n",
        "    print(\"type of pr_mask\",type(pr_mask))\n",
        "    #print(pr_mask[0])\n",
        "    #print(\"x shape = {0}\".format(x_tensor.shape))\n",
        "    print (\"shape of pr mask\", pr_mask.shape)\n",
        "    #print (\"shape of gt mask\", gt_mask.shape)\n",
        "    #print (\"shape of pr mask1\", pr_mask1.shape)\n",
        "\n",
        "    #print(\"unique values of pr mask\", np.unique(pr_mask))\n",
        "    #print(\"unique values of gt mask\", np.unique(gt_mask))\n",
        "    #print(\"unique values of pr mask\", np.unique(pr_mask1))\n",
        "    #print(\"unique values of pr mask1\", np.unique(pr_mask1))\n",
        "\n",
        "    #want to save the array as an image#\n",
        "    img1 = pr_mask.squeeze()\n",
        "    print (\"shape of img1 after squeeze\", img1.shape)\n",
        "    print (\"type of img1 after squeeze\", type(img1))\n",
        "    # we are only saving 1 class at a time. since we want obstacles, i have put 3 in the array below#\n",
        "    save_image(img1[3], 'img1.png')\n",
        "    \n",
        "    #CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "    #              0.      1.       2.        3             4                 5           6\n",
        "    visualize(\n",
        "        image=image_vis, \n",
        "        ground_truth_mask=gt_mask[3], \n",
        "        predicted_mask=pr_mask1[3]\n",
        "    )\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e060974d1222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name of the image being predicted\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimage_vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset_vis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu0LcU_dladE"
      },
      "source": [
        "'''\n",
        "mask_image = cv2.imread(\"/content/img1.png\", cv2.IMREAD_UNCHANGED)\n",
        "print(np.unique(mask_image))\n",
        "print(mask_image)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}